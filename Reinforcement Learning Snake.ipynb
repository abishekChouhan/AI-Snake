{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageGrab #grabbing image\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (30, 30)\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#keras imports\n",
    "%matplotlib inline \n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"game/snake.html\"\n",
    "chrome_driver_path = \"../chromedriver.exe\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Module\n",
    "This is the main module that implements interfacing between the python and browser-javascript using selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        self._driver = webdriver.Chrome(executable_path = \"chromedriver.exe\",chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(400,500)\n",
    "        self._driver.get(os.path.abspath(game_url))\n",
    "        #modifying game before trainNetworkining\n",
    "#         if custom_config:\n",
    "#             self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Init.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Init.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ENTER)\n",
    "        \n",
    "        time.sleep(0.25)# no actions are possible \n",
    "                        # for 0.25 sec after game starts, \n",
    "                        # skip learning at this time and make the model wait\n",
    "    def press_enter(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ENTER)\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def press_left(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_LEFT)\n",
    "    def press_right(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_RIGHT)\n",
    "        \n",
    "    def get_score(self):\n",
    "#         score_array = self._driver.execute_script(\"return Init.instance_.score\")\n",
    "        score = self._driver.execute_script(\"return Init.instance_.score\")\n",
    "        return int(score)\n",
    "#     def pause(self):\n",
    "#         return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "#     def resume(self):\n",
    "#         return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent module\n",
    "This model represent the agent (Dino) which the model controls for playing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeAgent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.start(); #to start the game, we need to jump once\n",
    "        time.sleep(.5) # no action can be performed for the first time when game starts\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def up(self):\n",
    "        self._game.press_up()\n",
    "    def start(self):\n",
    "        self._game.press_enter()\n",
    "    def down(self):\n",
    "        self._game.press_down()\n",
    "    def left(self):\n",
    "        self._game.press_left()\n",
    "    def right(self):\n",
    "        self._game.press_right()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game state module\n",
    "Game state helps to get the current state of the game environment as well as the agent.<br>\n",
    "Actions are performed by this model before getting a new state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "#         actions_df.loc[len(actions_df)] = actions[1:]\n",
    "        print(actions[1:])# storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.01*score - 0.01 # dynamic reward calculation\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.left()\n",
    "#             actions = [0,-1,-1,0,0]\n",
    "            reward = 0.1*score\n",
    "        elif actions[2] == 1:\n",
    "            self._agent.right()\n",
    "#             actions = [0,-1,-1,0,0]\n",
    "            reward = 0.1*score\n",
    "        elif actions[3] == 1:\n",
    "            self._agent.up()\n",
    "#             actions = [0,0,0,-1,-1]\n",
    "            reward = 0.1*score\n",
    "        elif actions[4] == 1:\n",
    "            self._agent.down()\n",
    "#             actions = [0,0,0,-1,-1]\n",
    "            reward = 0.1*score\n",
    "        image = grab_screen() \n",
    "        self._display.send(image) #display the image on screen\n",
    "\n",
    "        if self._agent.is_crashed():\n",
    "#             scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            if score == 0:\n",
    "                reward = 0.0\n",
    "            else:\n",
    "                reward = score\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "def grab_screen(_driver = None):\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(0,210,380,430))) #bbox = region of interset on the entire screen\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    #game is already in grey scale canvas, canny to get only edges and reduce unwanted objects(clouds)\n",
    "    image = cv2.resize(image, (0,0), fx = 0.4, fy = 0.4) # resale image dimensions\n",
    "#     image = image[2:38,10:50] #img[y:y+h, x:x+w] #crop out the dino agent from the frame\n",
    "    image = cv2.Canny(image, threshold1 = 100, threshold2 = 200) #apply the canny edge detection\n",
    "    return  image\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # g = Game()\n",
    "# # s = SnakeAgent(g)\n",
    "# # Game_sate(s,g)\n",
    "# i = grab_screen()\n",
    "# i = process_img(i)\n",
    "# i = cv2.resize(i, (800, 400)) \n",
    "# cv2.imshow('window_title', i)\n",
    "# ImageGrab.grab(bbox=(0,210,380,430))\n",
    "# m = np.array(ImageGrab.grab(bbox=(0,210,380,430)))\n",
    "# m  = cv2.resize(m, (0,0), fx = 0.4, fy = 0.4)\n",
    "# m = cv2.Canny(m, threshold1 = 100, threshold2 = 200)\n",
    "# m = cv2.resize(m, (800, 400))\n",
    "# cv2.imwrite('color_img.jpg', m)\n",
    "# cv2.imshow(\"image\", m);\n",
    "# cv2.waitKey();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize log structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 5 # possible actions\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 1000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 152,88\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same',input_shape=(img_cols,img_rows,img_channels)))  #20*40*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "We finish building the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_106 (Conv2D)          (None, 22, 38, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 22, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_107 (Conv2D)          (None, 11, 19, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 11, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, 11, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 11, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 13376)             0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 512)               6849024   \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 6,929,573\n",
      "Trainable params: 6,929,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "buildmodel().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque() #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1 #0 =>do nothing, 1=>left, 2=>right, 3=>up, 4=>down\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "#     acts = [0,-1,-1,0,0]\n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "#         model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = INITIAL_EPSILON\n",
    "#         model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "    t=0\n",
    "#     t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "#                 while True:\n",
    "#                     action_index = random.randrange(ACTIONS)\n",
    "#                     if a_t[action_index] != -1\n",
    "#                         a_t[action_index] = 1\n",
    "#                         break\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # 0=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('reward: {}'.format(r_t))\n",
    "        print('loop took {} seconds'.format(time.time()-last_time)) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "        else:\n",
    "            # artificial time delay as training done with this delay\n",
    "            time.sleep(0.12)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0 and False:\n",
    "            print(\"Now we save model\")\n",
    "            \n",
    "            model.save_weights(\"model_final.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = SnakeAgent(game)\n",
    "    game_state = Game_sate(dino,game)\n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data_final_working.npy file contains the the keystrokes and gameframes recording for a score of 500\n",
    "supervised_frames = np.load(\"training_data_final_working.npy\")\n",
    "frame = supervised_frames[0][0]\n",
    "action_index = supervised_frames[0][1]\n",
    "#plotting a sample frame from human recorded gameplay\n",
    "plt.imshow(frame)\n",
    "print('Action taken at this frame : Action index = {} i.e. jump'.format(str(action_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_actions = []\n",
    "\n",
    "for frame in supervised_frames:\n",
    "    supervised_actions.append(frame[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1,nrows =2,figsize=(15,15))\n",
    "sns.distplot(supervised_actions,ax=axs[0])\n",
    "axs[1].set_title('AI gameplay distribution')\n",
    "axs[0].set_title('Human gameplay distribution')\n",
    "actions_df = pd.read_csv(\"./objects/actions_df.csv\")\n",
    "sns.distplot(actions_df,ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "1. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,\n",
    "Daan Wierstra,  and Martin Riedmiller. ‘Playing Atari with Deep Reinforcement Learning’ arXiv:1312.5602, 19 Dec 2013\n",
    "2. Kevin Chen, Deep Reinforcement Learning for Flappy Bird.\n",
    "3. Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58–68, 1995.\n",
    "4. Toy example of a deep reinforcement learning model playing a game of catching fruit, https://github.com/bitwise-ben/Fruit\n",
    "5. MNIH, Volodymyr, et al. Human-level control through deep reinforcement learning. Nature, 2015, vol. 518, no 7540, p. 529-533.\n",
    "6. Tambet Matiisen. Demystifying Deep Reinforcement Learning https://ai.intel.com/demystifying-deep-reinforcement-learning/\n",
    "7. Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch Reinforcement Learning\n",
    "8. Using Deep Q-Network to Learn How To Play Flappy Bird   https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "9. The image processing modules were inspired from Harrison Sentdex's github which is licensed under the GNU GENERAL PUBLIC LICENSE http://www.gnu.org/licenses/gpl.html<br>\n",
    "10. Coroutine implementation in the code below belongs to Ben Meijering and is licensed under the MIT License https://opensource.org/licenses/MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
