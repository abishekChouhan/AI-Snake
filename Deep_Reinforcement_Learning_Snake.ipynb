{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from PIL import ImageGrab #if windows or os X\n",
    "import pyscreenshot as ImageGrab  #if linux\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "#from matplotlib import pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = (30, 30)\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#keras imports\n",
    "#%matplotlib inline \n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"./snake.html\"\n",
    "chrome_driver_path = \"../chromedriver.exe\"\n",
    "loss_file_path = \"./data/loss_df.csv\"\n",
    "actions_file_path = \"./data/actions_df.csv\"\n",
    "scores_file_path = \"./data/scores_df.csv\"\n",
    "time_file_path = \"./data/time_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(loss_file_path):\n",
    "    loss_df = pd.read_csv(loss_file_path)\n",
    "else:\n",
    "    loss_df = pd.DataFrame(columns =['loss'])\n",
    "    f = open(loss_file_path, \"w+\")\n",
    "    loss_df.to_csv(loss_file_path, index=False)\n",
    "\n",
    "if os.path.isfile(scores_file_path):\n",
    "    scores_df = pd.read_csv(scores_file_path)\n",
    "else:\n",
    "    scores_df = pd.DataFrame(columns = ['scores'])\n",
    "    open(scores_file_path, \"x\")\n",
    "    scores_df.to_csv(scores_file_path, index=False)\n",
    "\n",
    "if os.path.isfile(actions_file_path):\n",
    "    actions_df = pd.read_csv(actions_file_path)\n",
    "else:\n",
    "    actions_df = pd.DataFrame(columns = ['left', 'right', 'up', 'down'])\n",
    "    open(actions_file_path, \"x\")\n",
    "    actions_df.to_csv(actions_file_path, index=False)\n",
    "\n",
    "if os.path.isfile(time_file_path):\n",
    "    time_df = pd.read_csv(time_file_path)\n",
    "else:\n",
    "    time_df = pd.DataFrame(columns = ['time'])\n",
    "    open(time_file_path, \"x\")\n",
    "    time_df.to_csv(time_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Module\n",
    "This is the main module that implements interfacing between the python and browser-javascript using selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo apt-get install chromium-chromedriver\n",
    "\n",
    "img_rows, img_cols = 64,64\n",
    "SPEED = 360\n",
    "WIDTH = 8\n",
    "HEIGHT = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,speed, width, height):\n",
    "        self.speed = speed\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        self._driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "#         self._driver = webdriver.Chrome(executable_path = \"chromedriver.exe\",chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(img_rows*5+50,img_cols*5+50)\n",
    "        self._driver.get(\"file://\"+os.path.abspath(game_url))\n",
    "        self._driver.execute_script(\"Init.instance_.speed=\"+str(self.speed))\n",
    "        print(self.width)\n",
    "        self._driver.execute_script(\"Init.instance_.width=\"+str(self.width))\n",
    "        self._driver.execute_script(\"Init.instance_.height=\"+str(self.height))\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Init.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Init.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ENTER)\n",
    "        \n",
    "        time.sleep(0.25)# no actions are possible \n",
    "                        # for 0.25 sec after game starts, \n",
    "                        # skip learning at this time and make the model wait\n",
    "    def press_enter(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ENTER)\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def press_left(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_LEFT)\n",
    "    def press_right(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_RIGHT)\n",
    "    def get_time(self):\n",
    "        time = self._driver.execute_script(\"return Init.instance_.time\")\n",
    "        return int(time)\n",
    "    def get_score(self):\n",
    "        score = self._driver.execute_script(\"return Init.instance_.score\")\n",
    "        return int(score)\n",
    "    def get_just_eaten(self):\n",
    "        just_eaten = self._driver.execute_script(\"return Init.instance_.just_eaten\")\n",
    "        return just_eaten\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent module\n",
    "This model represent the agent (Dino) which the model controls for playing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,game):\n",
    "        self._game = game\n",
    "        self.start()\n",
    "        time.sleep(.5)\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def is_just_eaten(self):\n",
    "        return self._game.get_just_eaten()\n",
    "    def up(self):\n",
    "        self._game.press_up()\n",
    "    def start(self):\n",
    "        self._game.press_enter()\n",
    "    def down(self):\n",
    "        self._game.press_down()\n",
    "    def left(self):\n",
    "        self._game.press_left()\n",
    "    def right(self):\n",
    "        self._game.press_right()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img()\n",
    "        self._display.__next__() \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = list(actions)\n",
    "        score = self._game.get_score() \n",
    "        time = self._game.get_time()\n",
    "        is_eaten = self._game.get_just_eaten()\n",
    "        reward = 0.01*score #- 0.01 + 0.01*time\n",
    "        if is_eaten:\n",
    "            reward = 1\n",
    "        is_over = False\n",
    "        if actions[0] == 1:\n",
    "            self._agent.left()\n",
    "            #reward = 0.1*score #+ 0.01*time\n",
    "        elif actions[1] == 1:\n",
    "            self._agent.right()\n",
    "            #reward = 0.1*score #+ 0.01*time\n",
    "        elif actions[2] == 1:\n",
    "            self._agent.up()\n",
    "            #reward = 0.1*score #+ 0.01*time\n",
    "        elif actions[3] == 1:\n",
    "            self._agent.down()\n",
    "            #reward = 0.1*score #+ 0.01*time\n",
    "        image = grab_screen() \n",
    "        self._display.send(image)\n",
    "\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(scores_df)] = score\n",
    "            time_df.loc[len(time_df)] = time\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "#             if score == 0:\n",
    "#                 reward = -1*(0.0 + 0.01*time)\n",
    "#             else:\n",
    "#                 reward = -1*(score + 0.01*time)\n",
    "            is_over = True\n",
    "        return image, reward, is_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('data/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "def grab_screen(_driver = None):\n",
    "    x = 80\n",
    "    y = 240\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(x,y,x+WIDTH*14,y+HEIGHT*14))) #bbox = region of interset on the entire screen\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    #image = cv2.resize(image, (0,0), fx = 0.3, fy = 0.3) # resale image dimensions\n",
    "    #image = cv2.Canny(image, threshold1 = 10, threshold2 = 20)\n",
    "    image = cv2.resize(image, (img_rows,img_cols))\n",
    "    image = cv2.Canny(image, threshold1 = 10, threshold2 = 20)\n",
    "    # image = cv2.resize(image, (img_rows-10,img_cols-10))\n",
    "    return  image\n",
    "def show_img(graphs = False):\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (200, 100)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = 4 # possible actions\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 10000. # timesteps to observe before training\n",
    "EXPLORE = 30000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.00001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 64 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_channels = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (7, 7), strides=(4, 4), padding='same',input_shape=(img_cols,img_rows,img_channels)))  #20*40*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buildmodel().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    D = deque()\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "#     do_nothing[0] = 1 #0 =>do nothing, 1=>left, 2=>right, 3=>up, 4=>down\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "#     print('x_t: {}'.format(x_t.shape))\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "#     print('s_t: {}'.format(s_t.shape))\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "#     print('s_t reshaped: {}'.format(s_t.shape))\n",
    "    initial_state = s_t \n",
    "    model.save_weights(\"model_final.h5\")\n",
    "    \n",
    "    if observe :\n",
    "        OBSERVE = 999999999\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = INITIAL_EPSILON\n",
    "        model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "    t=0\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            if  random.random() <= epsilon:\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)\n",
    "                max_Q = np.argmax(q)\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1\n",
    "                \n",
    "        #Reduced the epsilon\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('reward: {}'.format(r_t))\n",
    "        #print('loop took {} seconds'.format(time.time()-last_time))\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1)\n",
    "#         print('x_t1: {}'.format(x_t1.shape))\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "#         print('s_t1: {}'.format(s_t1))\n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            print(len(D), BATCH)\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 2D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t\n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      # predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "        else:\n",
    "            time.sleep(0.10)\n",
    "        s_t = initial_state if terminal else s_t1 \n",
    "        t = t + 1\n",
    "        \n",
    "        if t % 500 == 0:\n",
    "            print(\"Save model\")\n",
    "            model.save_weights(\"model_final.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\")\n",
    "            save_obj(t,\"time\")\n",
    "            save_obj(epsilon,\"epsilon\")\n",
    "            loss_df.to_csv(loss_file_path,index=False)\n",
    "            print(scores_df.tail())\n",
    "            scores_df.to_csv(scores_file_path,index=False)\n",
    "            time_df.to_csv(time_file_path,index=False)\n",
    "            actions_df.to_csv(actions_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,\"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame(observe=False):\n",
    "    game = Game(speed=SPEED, width=WIDTH, height=HEIGHT)\n",
    "    dino = Agent(game)\n",
    "    game_state = State(dino,game)\n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cache()\n",
    "# actions = np.array([0.0,0.0,0.0,0.0,0.0])\n",
    "# actions_df.loc[len(actions_df)] = list(actions[1:])\n",
    "# print(actions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([[1,2,3],[5,8,3],[3,4,5]])\n",
    "# s = np.stack((x,x), axis=2)\n",
    "# # print(s)\n",
    "# s = s.reshape(1, s.shape[0], s.shape[1], s.shape[2])\n",
    "# print(s)\n",
    "# x2 = np.array([[5,43,5],[12,34,78],[326,78,54]])\n",
    "# x_ = x2.reshape(1, x2.shape[0], x2.shape[1], 1)\n",
    "\n",
    "# s_ = np.append(x_, s[:, :, :, :1], axis=3)\n",
    "# print(s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Now we build the model\n",
      "We finish building the model\n",
      "reward: 0.0\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 1\n",
      "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 118 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 119 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 120 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 121 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 122 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 123 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 124 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 125 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 126 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 127 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 128 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 129 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 130 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 131 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 132 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 133 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 134 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 135 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 136 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 137 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 138 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 139 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 140 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 141 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 142 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 143 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 144 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 145 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 146 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 147 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 148 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 149 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 150 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 151 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 152 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 153 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 154 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 155 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 156 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 157 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 158 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 159 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 160 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1\n",
      "TIMESTEP 161 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 162 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 163 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 164 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 165 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 166 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 167 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 168 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 169 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 170 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 171 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 172 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 1\n",
      "TIMESTEP 173 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 174 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 175 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 176 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 177 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 178 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 179 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 180 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 181 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 182 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 183 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 184 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 185 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 186 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 1\n",
      "TIMESTEP 187 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 188 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 189 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 190 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 191 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 192 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 193 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 194 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 195 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 196 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 197 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 198 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 199 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 200 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 201 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 202 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 203 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 204 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 205 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 206 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 207 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 208 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 209 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 210 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 211 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 212 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 213 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 214 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 215 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 216 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 217 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 218 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 219 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 220 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 221 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 222 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 223 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 224 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 225 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 226 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 227 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 228 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 229 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 230 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 231 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 232 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 233 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 234 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 235 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 236 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 237 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 238 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 239 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "TIMESTEP 240 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 241 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 242 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 243 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 244 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 245 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 246 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 247 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 248 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 249 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 250 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 251 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 252 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 253 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 254 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 255 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 256 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 257 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 258 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 259 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 260 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 261 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 262 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 263 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 264 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 265 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 266 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 267 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 268 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 269 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 270 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 271 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 272 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 273 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 274 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 275 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 276 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 277 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 278 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 279 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 280 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 281 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 282 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 283 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 284 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 285 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 286 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 287 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 288 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 289 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 290 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 291 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 292 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 293 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 294 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 295 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 296 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 297 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 298 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 299 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 300 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 301 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 302 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 303 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 304 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 305 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 306 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 307 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 308 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 309 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 310 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 1\n",
      "TIMESTEP 311 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 312 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: 0.01\n",
      "TIMESTEP 313 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.01 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 314 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 315 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 316 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 317 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 318 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 319 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "TIMESTEP 320 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 321 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 322 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 323 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 324 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 325 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 326 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 327 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 328 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 329 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 330 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 331 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 332 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 333 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 334 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 335 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 336 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 337 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 338 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 339 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 340 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 341 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 342 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 343 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 344 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 345 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 346 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 347 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 348 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 349 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 350 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 351 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 352 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 353 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 354 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 355 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 356 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 357 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 358 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 359 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 360 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 361 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 362 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 363 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 364 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 365 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 366 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 367 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 368 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 369 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 370 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 371 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 372 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 373 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 374 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 375 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 376 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 377 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 378 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 379 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 380 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 381 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 382 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 383 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 384 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 385 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 386 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 387 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 388 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 389 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 390 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 391 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 392 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 393 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 394 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 395 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 396 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 397 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 398 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 399 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 400 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 401 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "TIMESTEP 402 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 403 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 404 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 405 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 406 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 407 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 408 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 409 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 410 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 411 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 412 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 413 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 414 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 415 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 416 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 417 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 418 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 419 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 420 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 421 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 422 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 423 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 424 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 425 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 426 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 427 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 428 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 429 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 430 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 431 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 432 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 433 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 434 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 435 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 436 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 437 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 438 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 439 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 440 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 441 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 442 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 443 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 444 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 445 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 446 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 447 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 448 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 449 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 450 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 451 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 452 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 453 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 454 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 455 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 456 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 457 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 458 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 459 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 460 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 461 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 462 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 463 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 464 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 465 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 466 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 467 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 468 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 469 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 470 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 471 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 472 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 473 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 474 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 475 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 476 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 477 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 478 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 479 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 480 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 481 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "TIMESTEP 482 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 483 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 484 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 485 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 486 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 487 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 488 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 489 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 490 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 491 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 492 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 493 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 494 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 495 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 496 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 497 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 498 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 499 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "Save model\n",
      "      scores\n",
      "6716       0\n",
      "6717       0\n",
      "6718       0\n",
      "6719       0\n",
      "6720       0\n",
      "TIMESTEP 500 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 501 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 502 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 503 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 504 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 505 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 506 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 507 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 508 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 509 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 510 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 511 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 512 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 513 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 514 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 515 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 516 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 517 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 518 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 519 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 520 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 521 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 522 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 523 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 524 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 525 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 526 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 527 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 528 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 529 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 530 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 531 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 532 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 533 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 534 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 535 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 536 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 537 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 538 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 539 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 540 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 541 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 542 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 543 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 544 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 545 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 546 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 547 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 548 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 549 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 550 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 551 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 552 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 553 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 554 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 555 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 556 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 557 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 558 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 559 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 560 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 561 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1\n",
      "TIMESTEP 562 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 563 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 564 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 565 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 566 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 567 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 568 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 569 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 570 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 571 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 572 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 573 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 574 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 575 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 576 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 577 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 578 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 579 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 580 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 581 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 582 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 583 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 584 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 585 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 586 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 587 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 588 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 589 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 590 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 591 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 592 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 593 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 594 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 595 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 596 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 597 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 598 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 599 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 600 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 601 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 602 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 603 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 604 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 605 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 606 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 607 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 608 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 609 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 610 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 611 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 612 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 613 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 614 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 615 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 616 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 617 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 618 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 619 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 620 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 621 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 622 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 623 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 624 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 625 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 626 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 627 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 628 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 629 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 630 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 631 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 632 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 633 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 634 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 635 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 636 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 637 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 638 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 639 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 640 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "TIMESTEP 641 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 642 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 643 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 644 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 645 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 646 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 647 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 648 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 649 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 650 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 651 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 652 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 653 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: -1\n",
      "TIMESTEP 654 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 655 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 656 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 657 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 658 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 659 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 660 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 661 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 662 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 663 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 664 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 665 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 666 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 667 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 668 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 669 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 670 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 671 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 672 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 673 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 674 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 675 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 676 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 677 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 678 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 679 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 680 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 681 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 682 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 683 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 684 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 685 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 686 / STATE observe / EPSILON 0.1 / ACTION 3 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 687 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 688 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 689 / STATE observe / EPSILON 0.1 / ACTION 2 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 690 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 691 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 692 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 693 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 694 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 695 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 696 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 697 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 698 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 699 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 700 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "reward: 0.0\n",
      "TIMESTEP 701 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: -1\n",
      "TIMESTEP 702 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 703 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 704 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 705 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 706 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 707 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "reward: 0.0\n",
      "TIMESTEP 708 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4829a0a02061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-d08dd8af195f>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(observe)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-bec043155243>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(model, game_state, observe)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#run the selected action and observed next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mx_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reward: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#print('loop took {} seconds'.format(time.time()-last_time))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-b2e0b4185340>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#reward = 0.1*score #+ 0.01*time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrab_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-1c683d1bf9cc>\u001b[0m in \u001b[0;36mgrab_screen\u001b[0;34m(_driver)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m240\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImageGrab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mWIDTH\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#bbox = region of interset on the entire screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#processing image as required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyscreenshot/__init__.py\u001b[0m in \u001b[0;36mgrab\u001b[0;34m(bbox, childprocess, backend)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mchildprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchildprocess_default_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     return _grab(\n\u001b[0;32m---> 67\u001b[0;31m         to_file=False, childprocess=childprocess, backend=backend, bbox=bbox)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyscreenshot/__init__.py\u001b[0m in \u001b[0;36m_grab\u001b[0;34m(to_file, childprocess, backend, bbox, filename)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running \"%s\" in child process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         return run_in_childprocess(\n\u001b[0;32m---> 46\u001b[0;31m             _grab_simple, imcodec.codec, to_file, backend, bbox, filename)\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_grab_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyscreenshot/procutil.py\u001b[0m in \u001b[0;36mrun_in_childprocess\u001b[0;34m(target, codec, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_frames = np.load(\"training_data_final_working.npy\")\n",
    "frame = supervised_frames[0][0]\n",
    "action_index = supervised_frames[0][1]\n",
    "plt.imshow(frame)\n",
    "print('Action taken at this frame : Action index = {} i.e. jump'.format(str(action_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_actions = []\n",
    "\n",
    "for frame in supervised_frames:\n",
    "    supervised_actions.append(frame[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1,nrows =2,figsize=(15,15))\n",
    "sns.distplot(supervised_actions,ax=axs[0])\n",
    "axs[1].set_title('AI gameplay distribution')\n",
    "axs[0].set_title('Human gameplay distribution')\n",
    "actions_df = pd.read_csv(\"./data/actions_df.csv\")\n",
    "sns.distplot(actions_df,ax=axs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
